{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ad9a12",
   "metadata": {},
   "source": [
    "# (1) Twitter Data\n",
    "## (1.1) Getting Twitter data 2021 from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e87ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from path import Path\n",
    "from twarc import Twarc2, expansions\n",
    "import json\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7095c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import bearer_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f34565",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Twarc2(bearer_token=bearer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'elonmusk'\n",
    "posts_dict = {\n",
    "    'date':[],\n",
    "    'text':[],\n",
    "    'like_count':[],\n",
    "    'reply_count':[],\n",
    "    'retweet_count':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60457707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull posts from Twitter and create a dictionary\n",
    "user_timeline = client.timeline(user=user, exclude_replies=True, start_time=datetime.datetime(2021,1,1, 0, 0, 0) )\n",
    "for page in user_timeline:\n",
    "    result = expansions.flatten(page)\n",
    "    for tweet in result:\n",
    "        posts_dict['date'].append(tweet['created_at'])\n",
    "        posts_dict['text'].append(tweet['text'])\n",
    "        posts_dict['like_count'].append(tweet['public_metrics']['like_count'])\n",
    "        posts_dict['reply_count'].append(tweet['public_metrics']['reply_count'])\n",
    "        posts_dict['retweet_count'].append(tweet['public_metrics']['retweet_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary of posts to dataframe\n",
    "twitter_2021 = pd.DataFrame.from_dict(posts_dict)\n",
    "twitter_2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to datetime datatype\n",
    "twitter_2021['date'] = pd.to_datetime(twitter_2021['date']).dt.date.astype('datetime64')\n",
    "twitter_2021.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1164c",
   "metadata": {},
   "source": [
    "## (1.2) Getting Twitter data 2011 - 2020 from archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load twitter data from csv file\n",
    "file_to_load = os.path.join('Data', 'elon_musk_tweets_2011-2021.csv')\n",
    "twitter_archive = pd.read_csv(file_to_load)\n",
    "twitter_archive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95bfbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select and rename columns\n",
    "twitter_archive_clean = twitter_archive[['date', 'tweet', 'nlikes', 'nreplies', 'nretweets']]\\\n",
    "                            .loc[(twitter_archive['reply_to'] == '[]') & (twitter_archive['retweet'] == False)]\n",
    "twitter_archive_clean.columns=['date', 'text', 'like_count', 'reply_count', 'retweet_count']\n",
    "\n",
    "# convert date to datetime datatype\n",
    "twitter_archive_clean['date'] = pd.to_datetime(twitter_archive_clean['date']).dt.date.astype('datetime64')\n",
    "\n",
    "# drop last row with 1 tweet in 2011\n",
    "twitter_archive_clean.drop(twitter_archive_clean.tail(1).index,inplace=True)\n",
    "\n",
    "twitter_archive_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c62e0",
   "metadata": {},
   "source": [
    "## (1.3) Clean the twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b87bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate 2 datasets to get tweets from 2011 to 2021\n",
    "twitter_df_merged = pd.concat([twitter_2021, twitter_archive_clean])\n",
    "twitter_df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971be6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the NaNs\n",
    "twitter_df_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3abdaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export all tweets for analysis in Tableau\n",
    "twitter_df_merged.to_csv('Data/tweets_data_2011_2021_ungrouped.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23122e",
   "metadata": {},
   "source": [
    "## (1.4) Preprocessing the Twitter data\n",
    "\n",
    "**Preprocess the data by making it all lowercase. Remove a reasonable set of stopwords from the dataset and tokenize. Then, report the 10 most common words and their count. We need to iterate this process, adding some stop words as we understand the structure of the data. Justify additional stop words we've added.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8530984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from datetime import datetime\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731df31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# group tweets that posted at the same day\n",
    "def f(x):\n",
    "     return pd.Series(dict(like_count = x['like_count'].sum(),\n",
    "                        reply_count = x['reply_count'].sum(),\n",
    "                        retweet_count = x['retweet_count'].sum(),\n",
    "                        text = \"{%s}\" % ', '.join(x['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d661c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df_merged = twitter_df_merged.groupby('date').apply(f).reset_index()\n",
    "twitter_df_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a68df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df_merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57afb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter_df_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-processing and make the tweets all lowercase and remove stopwords.\n",
    "# lower the tweets\n",
    "twitter_df['preprocessed_text'] = twitter_df['text'].str.lower()\n",
    "\n",
    "# remove apostrophe from words and url\n",
    "twitter_df['preprocessed_text'] = [re.sub(\"('[a-z]+)\\s\", \" \", row) for row in twitter_df['preprocessed_text']]\n",
    "twitter_df['preprocessed_text'] = [re.sub(\"(')\\s\", \" \", row) for row in twitter_df['preprocessed_text']]\n",
    "twitter_df['preprocessed_text'] = [re.sub(\"(?:https:\\/\\/\\S+)\\s\", \"\", row) for row in twitter_df['preprocessed_text']]\n",
    "\n",
    "                                      \n",
    "# filter out rest URLs\n",
    "url_re = '(?:https?:\\/\\/)?(?:[^?\\/\\s]+[?\\/])(.*)'\n",
    "twitter_df['preprocessed_text'] = twitter_df['preprocessed_text'].apply(lambda row: ' '.join([word for word in row.split() if (not re.match(url_re, word))]))\n",
    "\n",
    "# tokenize the tweets\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z]\\w+\\'?\\w*')\n",
    "twitter_df['tokenized_text'] = twitter_df['preprocessed_text'].apply(lambda row: tokenizer.tokenize(row))\n",
    "\n",
    "#create an object of class PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# apply stemming\n",
    "twitter_df['preprocessed_text'] = [porter.stem(row) for row in twitter_df['preprocessed_text']]   \n",
    "\n",
    "# filter out stop words\n",
    "en_stop_words = nltk.corpus.stopwords.words('english')\n",
    "additional_stop_words =['amp', 'rt', 'th','co', 're', 've', 'kim', 'daca', 'us', 'it', 'th', 'you', 'haha', 'st', 'et', 'so', 'iii', 'also', 've', 'la', 're', 'the', 'https', 'wow', 'actually', 'due', 'ft', 'pcr', 'via', 'am', 'gt', 'com', 'since', 'in', 'me', 'and', 'btw', 'yesterday', 'ii', 'inu', 'on', 'http', 'to', 'vs', 'rd', 'ur', 'of', 'bs', 'km', 'est', 'em', 'lz', 'kms', 'aft', 'nd',  'hereâ€™s', 're', 'mqxfakpzf' 'mph', 'ht', 'etc', 'dm', 'doo']\n",
    "en_stop_words.extend(additional_stop_words)\n",
    "\n",
    "twitter_df['tokenized_text'] = twitter_df['tokenized_text'].apply(lambda row: [word for word in row if (not word in en_stop_words)])\n",
    "\n",
    "df_tweets_clean = twitter_df.copy()\n",
    "df_tweets_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba4f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tweets_clean = df_tweets_clean[['date', 'text', 'tokenized_text', 'like_count', 'reply_count', 'retweet_count']]\n",
    "df_tweets_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique words\n",
    "def get_most_freq_words(str, n=None):\n",
    "    vect = CountVectorizer().fit(str)\n",
    "    bag_of_words = vect.transform(str)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n",
    "    freq =sorted(freq, key = lambda x: x[1], reverse=True)\n",
    "    return freq[:n]\n",
    "  \n",
    "len(get_most_freq_words([ word for tweet in df_tweets_clean.tokenized_text for word in tweet]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effaa3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_csv('data/tweets_data_2011_2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10cef55",
   "metadata": {},
   "source": [
    "## (1.5) Upload dataset to SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdcd9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import psycopg2\n",
    "from config import user, password, hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{hostname}/twitter_vs_stocks')\n",
    "\n",
    "# Use the Inspector to explore the database\n",
    "inspector = inspect(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76530bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_sql('tweets_text', engine, if_exists ='replace',method='multi', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
